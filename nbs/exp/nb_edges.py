
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nb/edges.ipynb
from torch import Tensor, randint
import torch.nn as nn
from ..layer import Flatten
from ..function import ceil_power_of_two
from ..constants import NOISE_RATIO, COPY_RATIO
import numpy as np
from math import ceil

class Edge:
    def __init__(self, src, dest, layer=None, identical=False):
        self.src = src
        self.dest = dest
        self.identical = identical
        if not identical:
            self.layer = layer

    def as_layer(self):
        return self.layer

    def updated_src(self, in_shape, out_shape, expanded=[]):
        return out_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        return in_shape, expanded

    def calculate_output(self, in_shape):
        return in_shape

    def verify_output(self, in_shape, out_shape):
        expected_output = self.calculate_output(in_shape)
        if expected_output == None: return False
        if len(expected_output) != len(out_shape): return False
        for e, o in zip(expected_output, out_shape):
            if e > 0 and o > 0:
                if e != o: return False
        return True

    def set_weight(self, **kwargs):
        return

    @property
    def edge_args(self):
        return {'src': self.src, 'dest': self.dest,
                'layer': self.layer_name,
                'args': self.args}

    @property
    def layer_name(self):
        return 'undefined'

def add_noise(weight):
    noise_range = np.ptp(weight.flatten()) * NOISE_RATIO
    noise = np.random.uniform(-noise_range/2.0, noise_range/2.0, weight.shape)
    return np.add(weight, noise).float()

class FlattenEdge(Edge):
    def __init__(self, src, dest, **kwargs):
        self.args = {}
        super(FlattenEdge, self).__init__(src, dest, layer=Flatten(), identical=False)

    def updated_src(self, in_shape, out_shape, expanded=[]):
        out_shape = self.calculate_output(in_shape)
        feature_size = out_shape[0] // in_shape[0]

        new_expanded = []
        for o, c in expanded:
            if o < 0:
                origin = (-1,) * feature_size
            else:
                origin = range(o*feature_size, (o+1)*feature_size)
            copied = range(c*feature_size, (c+1)*feature_size)
            new_expanded.extend((o, c) for o, c in zip(origin, copied))

        return out_shape, new_expanded

    def calculate_output(self, in_shape):
        total = 1
        nf = in_shape[0]
        for i in in_shape:
            total = total * i
        out_shape = (total,)
        return out_shape

    @property
    def layer_name(self):
        return 'flatten'

class BatchNormEdge(Edge):

    def __init__(self, src, dest, num_features, **kwargs):
        self.args = {
            'num_features': num_features,
        }
        super(BatchNormEdge, self).__init__(src,
                                          dest,
                                          layer=nn.BatchNorm2d(**self.args),
                                          identical=False)
        self.layer.reset_parameters()

    def set_zeros(self):
        nn.init.constant_(self.layer.weight, 0)

    def set_identical(self):
        self.layer.reset_running_stats()
        self.layer.reset_parameters()
        nn.init.constant_(self.layer.bias, 0)
        nn.init.uniform_(self.layer.weight, a=1.0-NOISE_RATIO, b=1.0+NOISE_RATIO)

    def set_weight(self, **kwargs):
        self.layer.bias.data = kwargs['bias']
        self.layer.weight.data = kwargs['weight']
        return

    def updated_src(self, in_shape, out_shape, expanded=[]):

        nf = in_shape[0]
        extended_params = {
            'weight': Tensor(nf),
            'bias': Tensor(nf)
        }

        # Get original parameters.
        params = dict((name, value.data) for (name, value) in self.layer.named_parameters())

        # Expand original parameters
        for key in params.keys():
            extended_params[key][:len(params[key])] = params[key]
            for o, c in expanded:
                if o < 0:
                    extended_params[key][c].fill_(0.)
                else:
                    extended_params[key][c] = extended_params[key][o]
            extended_params[key] = add_noise(extended_params[key])

        # Update argument
        self.args['num_features'] = nf

        # Update the layer
        new_layer = nn.BatchNorm2d(**self.args)
        new_layer.weight.data = extended_params['weight']
        new_layer.bias.data = extended_params['bias']
        self.layer = new_layer

        # Generate new output shape
        out_shape = in_shape
        return out_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):

        nf = out_shape[0]
        extended_params = {
            'weight': Tensor(nf),
            'bias': Tensor(nf)
        }

         # Get original parameters.
        params = dict((name, value.data) for (name, value) in self.layer.named_parameters())

        # Expand original parameters
        for key in params.keys():
            extended_params[key][:len(params[key])] = params[key]
            for o, c in expanded:
                if o < 0:
                    extended_params[key][c].fill_(0.)
                else:
                    extended_params[key][c] = params[key][o]
            extended_params[key] = add_noise(extended_params[key])

        # Update argument
        self.args['num_features'] = nf

        # Update the layer
        new_layer = nn.BatchNorm2d(**self.args)
        new_layer.weight.data = extended_params['weight']
        new_layer.bias.data = extended_params['bias']
        self.layer = new_layer

        # Generate new input shape
        in_shape = out_shape
        return in_shape, expanded

    def calculate_output(self, in_shape):
        if in_shape[0] != self.args['num_features']: return None
        else: return in_shape

    @property
    def layer_name(self):
        return 'batchnorm'

class ReluEdge(Edge):
    def __init__(self, src, dest, **kwargs):
        self.args = {
        }
        super(ReluEdge, self).__init__(src,
                                       dest,
                                       layer=nn.ReLU(),
                                       identical=False)

    def updated_src(self, in_shape, out_shape, expanded=[]):
        return in_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        return out_shape, expanded

    @property
    def layer_name(self):
        return 'relu'

class PoolingEdge(Edge):
    def _get_layer(self):
        pass

    def __init__(self, src, dest, kernel_size, **kwargs):
        kernel_size = (kernel_size,) * 2 if isinstance(kernel_size, int) else kernel_size
        stride = kernel_size
        padding = (0,) * 2

        self.args = {
            'kernel_size': kernel_size,
            'stride': stride,
            'padding': padding,
            'ceil_mode': True
        }

        super(PoolingEdge, self).__init__(src,
                                          dest,
                                          layer=self._get_layer()(**self.args),
                                          identical=False)

    def updated_src(self, in_shape, out_shape, expanded=[]):
        out_shape = self.calculate_output(in_shape)

        return out_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        exp_shape = self.calculate_output(in_shape)
        if(out_shape[1:] != exp_shape[1:]):
            stride = tuple(ceil_power_of_two(i/o) for i, o in zip(in_shape[1:], out_shape[1:]))
            kernel_size = stride
            padding = (0,) * 2
            self.args['kernel_size'] = kernel_size
            self.args['padding'] = padding
            self.args['stride'] = stride

            self.layer = self._get_layer()(**self.args)

        in_shape = (out_shape[0],) + in_shape[1:]

        return in_shape, expanded

    def calculate_output(self, in_shape):
        nf = (in_shape[0],)
        attr = zip(in_shape[1:],
                   self.args['kernel_size'],
                   self.args['stride'],
                   self.args['padding'])
        ch = tuple([ceil((x + 2 * pd - ks) / st) + 1 for x, ks, st, pd in attr])
        out_shape = nf + ch
        return out_shape

class MaxPoolingEdge(PoolingEdge):
    def _get_layer(self):
        return nn.MaxPool2d

    @property
    def layer_name(self):
        return 'maxpooling'

class AvgPoolingEdge(PoolingEdge):
    def _get_layer(self):
        return nn.AvgPool2d

    @property
    def layer_name(self):
        return 'avgpooling'

class AdaptivePoolingEdge(Edge):
    def __init__(self, src, dest, output_size, **kwargs):
        pass

    def _set_args(self, output_size):
        self.args={
            'output_size': output_size
        }

    def updated_src(self, in_shape, out_shape, expanded=[]):
        return (in_shape[0],) + self.args['output_size'], expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        return (out_shape[0],) + in_shape[1:], expanded

    def calculate_output(self, in_shape):
        return (in_shape[0],) + self.args['output_size']

class AdaptiveMaxPoolingEdge(AdaptivePoolingEdge):
    def __init__(self, src, dest, output_size):
        self._set_args(output_size)
        super(AdaptivePoolingEdge, self).__init__(src,
                                                  dest,
                                                  layer=nn.AdaptiveMaxPool2d(**self.args),
                                                  identical=False)
    @property
    def layer_name(self):
        return 'adaptivemaxpooling'

class AdaptiveAvgPoolingEdge(AdaptivePoolingEdge):
    def __init__(self, src, dest, output_size):
        self._set_args(output_size)
        super(AdaptivePoolingEdge, self).__init__(src,
                                                  dest,
                                                  layer=nn.AdaptiveAvgPool2d(**self.args),
                                                  identical=False)
    @property
    def layer_name(self):
        return 'adaptiveavgpooling'

class LinearEdge(Edge):
    def __init__(self, src, dest, in_features, out_features, bias=True, **kwargs):
        self.args = {
            'in_features': in_features,
            'out_features': out_features,
            'bias': bias
        }
        super(LinearEdge, self).__init__(src,
                                         dest,
                                         layer=nn.Linear(**self.args),
                                         identical=False)
        self.layer.reset_parameters()

    def set_identical(self):
        if self.args['in_features'] != self.args['out_features']:
            raise Exception('Cannot set identical weight')
        nn.init.constant_(self.layer.weight.data, 0.)
        nn.init.constant_(self.layer.bias.data, 0.)

        ni = self.args['in_features']
        for i in range(ni):
            self.layer.weight.data[i,i] = 1

        self.layer.weight.data = add_noise(self.layer.weight.data)

    def set_weight(self, **kwargs):
        if self.args['bias']: self.layer.bias.data = kwargs['bias']
        self.layer.weight.data = kwargs['weight']
        return

    def updated_src(self, in_shape, out_shape, expanded=[]):
        ni = in_shape[0]

        # Get original parameters
        params = dict((name, value.data) for (name, value) in self.layer.named_parameters())

        # Expand original parameters
        expanded_weight = Tensor(self.args['out_features'], ni)
        expanded_weight[:,:self.args['in_features']] = params['weight']
        for o, c in expanded:
            if o < 0:
                expanded_weight[:,c].fill_(0.)
            else:
                expanded_weight[:,c] = params['weight'][:,o]
        expanded_weight = add_noise(expanded_weight)

        # Update the args
        self.args['in_features'] = ni

        # Update the layer
        new_layer = nn.Linear(**self.args)
        new_layer.weight.data = expanded_weight
        new_layer.bias.data = params['bias']
        self.layer = new_layer

        # Generate new output shape
        out_shape = self.calculate_output(in_shape)
        return out_shape, []

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        raise Excpetion('unimplemented')

    def calculate_output(self, in_shape):
        if in_shape[0] != self.args['in_features']: return None
        return (self.args['out_features'],)

    @property
    def layer_name(self):
        return 'linear'

class ConvEdge(Edge):
    def __init__(self, src, dest,
                 in_channels, out_channels, kernel_size, stride=1, bias=True, **kwargs):
        kernel_size = (kernel_size,) * 2 if isinstance(kernel_size, int) else kernel_size
        padding = tuple(ks//2 for ks in kernel_size)
        stride = (stride,) * 2 if isinstance(stride, int) else stride

        self.args = {
            'in_channels': in_channels,
            'out_channels': out_channels,
            'kernel_size': kernel_size,
            'stride': stride,
            'padding': padding,
            'bias': bias
        }
        super(ConvEdge, self).__init__(src,
                                       dest,
                                       layer=nn.Conv2d(**self.args),
                                       identical=False)
        self.layer.reset_parameters()

    def set_identical(self):
        if self.args['in_channels'] != self.args['out_channels']:
            raise Exception('Cannot set identical weight')
        center = tuple(i // 2 for i in self.args['kernel_size'])
        nf = self.args['in_channels']

        nn.init.constant_(self.layer.weight, 0.)
        if self.args['bias']: nn.init.constant_(self.layer.bias, 0.)
        for i in range(nf):
            self.layer.weight.data[i,i,center[0],center[1]] = 1.

        self.layer.weight.data = add_noise(self.layer.weight.data)

    def set_weight(self, **kwargs):
        if self.args['bias']: self.layer.bias.data = kwargs['bias']
        self.layer.weight.data = kwargs['weight']
        return

    def updated_src(self, in_shape, out_shape, expanded=[]):
        ni = in_shape[0]
        prev_ni = self.args['in_channels']
        nf = self.args['out_channels']

        # Update the args
        self.args['in_channels'] = ni

        # Adjust stride
        exp_shape = self.calculate_output(in_shape)
        if exp_shape[1:] != out_shape[1:]:
            stride = tuple(self._calculate_stride(i, o) for i, o in zip(in_shape[1:], out_shape[1:]))
            self.args['stride'] = stride

        # Get original parameters
        params = dict((name, value.data) for (name, value) in self.layer.named_parameters())

        # Expand original parameters
        expanded_weight = Tensor(nf, ni, *self.args['kernel_size'])
        expanded_weight[:, :prev_ni, ...] = params['weight']
        for o, c in expanded:
            if o < 0:
                expanded_weight[:, c, ...].fill_(0.)
            else:
                expanded_weight[:, c, ...] = params['weight'][:, o, ...]
        expanded_weight = add_noise(expanded_weight)

        # Update the layer
        new_layer = nn.Conv2d(**self.args)
        new_layer.weight.data = expanded_weight
        if self.args['bias']: new_layer.bias.data = params['bias']
        self.layer = new_layer

        # Generate new output shape
        out_shape = self.calculate_output(in_shape)
        return out_shape, []

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        ni = self.args['in_channels']
        nf = out_shape[0] if out_shape[0] > 0 else self.args['out_channels']
        prev_nf = self.args['out_channels']

        # Update the args
        self.args['out_channels'] = nf

        # Adjust stride
        exp_shape = self.calculate_output(in_shape)
        if exp_shape[1:] != out_shape[1:]:
            stride = tuple(ceil_power_of_two(i, o) for i, o in zip(in_shape[1:], out_shape[1:]))
            self.args['stride'] = stride

        # Get original parameters
        params = dict((name, value.data) for (name, value) in self.layer.named_parameters())

        # Expand original parameters
        expanded_params = {}
        expanded_params['weight'] = Tensor(nf, ni, *self.args['kernel_size'])
        if self.args['bias']: expanded_params['bias'] = Tensor(nf)

        rand = randint(prev_nf, (nf - prev_nf,))
        for key in params.keys():
            expanded_params[key][:prev_nf, ...] = params[key]

            for o, c in expanded:
                if o < 0:
                    expanded_params[key][c, ...].fill_(0.)
                else:
                    expanded_params[key][c, ...] = params[key][o, ...] * COPY_RATIO
                    expanded_params[key][o, ...] = params[key][o, ...] * (1 - COPY_RATIO)
            expanded_params[key] = add_noise(expanded_params[key])

        # Update the layer
        new_layer = nn.Conv2d(**self.args)
        new_layer.weight.data = expanded_params['weight']
        if self.args['bias']: new_layer.bias.data = expanded_params['bias']
        self.layer = new_layer

        # Generate new input shape
        in_shape = (ni,) + in_shape[1:]
        return in_shape, []

    def calculate_output(self, in_shape):
        if(in_shape[0] != self.args['in_channels']): return None
        nf = self.args['out_channels']
        attr = zip(in_shape[1:],
                   self.args['kernel_size'],
                   self.args['stride'],
                   self.args['padding'])
        ch = tuple([(x + 2 * pd - ks) // st + 1 for x, ks, st, pd in attr])
        out_shape = (nf,) + ch
        return out_shape

    @property
    def layer_name(self):
        return 'conv'

class DropoutEdge(Edge):
    def __init__(self, src, dest, p=0.5, **kwargs):
        self.args = {
            'p': p
        }
        super(DropoutEdge, self).__init__(src,
                                          dest,
                                          layer=nn.Dropout(**self.args),
                                          identical=False)

    def updated_src(self, in_shape, out_shape, expanded=[]):
        return in_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        return out_shape, expanded

    @property
    def layer_name(self):
        return 'dropout'

class IdenticalEdge(Edge):
    def __init__(self, src, dest, **kwargs):
        self.args = {}
        super(IdenticalEdge, self).__init__(src,
                                            dest,
                                            layer=None,
                                            identical=True)

    def updated_src(self, in_shape, out_shape, expanded=[]):
        return in_shape, expanded

    def updated_dest(self, in_shape, out_shape, expanded=[]):
        return out_shape, expanded

    @property
    def layer_name(self):
        return 'identical'