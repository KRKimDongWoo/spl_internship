{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from .graph.graph import Graph\n",
    "from .graph_transformer import *\n",
    "from .function import ceil_power_of_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from include.graph.graph import Graph\n",
    "from include.graph_transformer import *\n",
    "from include.function import ceil_power_of_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchored-Graph\n",
    "Add/Concat on anchors\n",
    "## Required functions\n",
    "    Add Anchor\n",
    "    Add threads\n",
    "    Verifies output and update\n",
    "## Limitations\n",
    "    Keep kernel size 1 or 3 or 5\n",
    "    Keep shape to ( x // 2**k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AnchorGraph(Graph):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(AnchorGraph, self).__init__(input_shape, output_shape)\n",
    "        self.anchor = []\n",
    "        self.convs = []\n",
    "        \n",
    "        next = add_conv_block(self, self.input, nf=64, ks=3, st=2)\n",
    "        self.convs.extend(next)\n",
    "        \n",
    "        next = self.add_anchor(next, layer=Add)\n",
    "        next = add_flatten_layer(self, next)\n",
    "        self.output = add_linear_layer(self, next, self.output)\n",
    "    \n",
    "    def _reconstruct(self, target, expanded=[], caller=None):\n",
    "        print('constructing on {}'.format(target))\n",
    "        is_concat = self.nodes[target].layer is Concat\n",
    "        nf = -1 if is_concat else self.nodes[target].shape[0] \n",
    "        out_shape = (nf,) + self.nodes[target].shape[1:]\n",
    "\n",
    "        # Update backwards.\n",
    "        for edge_id in self.nodes[target].in_edge:\n",
    "            edge = self.edges[edge_id]\n",
    "            \n",
    "            in_shape = self.nodes[edge.src].shape\n",
    "\n",
    "            print(in_shape, out_shape, edge.verify_output(in_shape, out_shape))\n",
    "            if edge.verify_output(in_shape, out_shape):\n",
    "                continue\n",
    "            else:\n",
    "                next_shape, next_expanded = edge.updated_dest(in_shape, out_shape, expanded)\n",
    "                self.nodes[edge.src].shape = next_shape\n",
    "                self._reconstruct(edge.src, next_expanded, target)\n",
    "                \n",
    "        # Update self\n",
    "        if is_concat:\n",
    "            offset = 0\n",
    "            for edge_id in self.nodes[target].in_edge:\n",
    "                edge = self.edges[edge_id]\n",
    "                if caller == edge.src:\n",
    "                    expanded = [\n",
    "                        (-1 if o < 0 else o + offset, c + offset) for (o, c) in expanded\n",
    "                    ]\n",
    "                offset = offset + self.nodes[edge.src].shape[0]\n",
    "            self.nodes[target].shape = (offset,) + self.nodes[target].shape[1:]\n",
    "\n",
    "        # Update forwards.\n",
    "        in_shape = self.nodes[target].shape\n",
    "        for edge_id in self.nodes[target].out_edge:\n",
    "            edge = self.edges[edge_id]\n",
    "            \n",
    "            is_concat = self.nodes[edge.dest].layer is Concat\n",
    "            nf = -1 if is_concat else self.nodes[edge.dest].shape[0]\n",
    "            out_shape = (nf,) + self.nodes[edge.dest].shape[1:]\n",
    "            \n",
    "            if edge.verify_output(in_shape, out_shape):\n",
    "                continue\n",
    "            else:\n",
    "                next_shape, next_expanded = edge.updated_src(in_shape, out_shape, expanded)\n",
    "                self.nodes[edge.dest].shape = next_shape\n",
    "                self._reconstruct(edge.dest, next_expanded, target)\n",
    "        \n",
    "    def add_anchor(self, src, layer):\n",
    "        anchor = self.insert_node(src, multi_input=True, layer=layer)\n",
    "        self.anchor.append(anchor)\n",
    "        return anchor\n",
    "    \n",
    "    def add_connection(self, src, dest, layer_features=[]):\n",
    "        in_shape = self.nodes[src].shape\n",
    "        out_shape = self.nodes[dest].shape\n",
    "        ni, nf = in_shape[0], out_shape[0]\n",
    "        \n",
    "        nh = ni\n",
    "        next = src\n",
    "        \n",
    "        nl = len(layer_features)\n",
    "        # Generate convolutional layers\n",
    "        for id, layer in enumerate(layer_features):\n",
    "            nh = layer if layer > 0 else nh\n",
    "            next = add_conv_block(self, next, nf=nh, ks=3, st=1,\n",
    "                                  zero_bn = True if id==nl-1 else False)\n",
    "        \n",
    "        # Match the output size\n",
    "        if in_shape[1:] != out_shape[1:]:\n",
    "            shapes = zip(in_shape[1:], out_shape[1:])\n",
    "            kernel = tuple(ceil_power_of_two(i/o) for i, o in shapes)\n",
    "            \n",
    "            next = add_pooling_layer(self, next, ks=kernel, method='avg')\n",
    "        \n",
    "        # Match the output channel\n",
    "        if self.nodes[dest].layer is Concat:\n",
    "            print(self.nodes[dest].layer)\n",
    "            nf = self.nodes[next].shape[0]\n",
    "            \n",
    "            id_edge = IdenticalEdge(next, dest)\n",
    "            self.add_edge(id_edge)\n",
    "            expanded = tuple((-1, x) for x in range(nf))\n",
    "            self._reconstruct(dest, expanded=expanded, caller=next)\n",
    "        else:\n",
    "            if ni != nf:\n",
    "                next = add_conv_layer(self, next,\n",
    "                                      nf=nf, ks=1, bias=True, identical=True)\n",
    "            id_edge = IdenticalEdge(next, dest)\n",
    "            self.add_edge(id_edge)\n",
    "        return dest\n",
    "    \n",
    "    def deeper_net(self, layer):\n",
    "        last_anchor = self.anchor[-1]\n",
    "        self.add_anchor(self.anchor[-1], layer=layer)\n",
    "        edge = ConvEdge(0, 0, in_channels=64, out_channels=64, kernel_size=3, bias=False)\n",
    "        edge.set_identical()\n",
    "        next = self.insert_node(last_anchor, edge=edge)\n",
    "        edge = BatchNormEdge(0, 0, 64)\n",
    "        edge.set_identical()\n",
    "        next = gr.insert_node(next, edge=edge)\n",
    "        next = gr.insert_node(next, edge=ReluEdge(0, 0))\n",
    "        return next\n",
    "    \n",
    "    def expand_channel(node, channel):\n",
    "        self.nodes[node].shape = shape\n",
    "        nf_prev = shape[0]\n",
    "        nf = channel\n",
    "        if nf_prev >= nf: raise Exception('Trying expand to lower channels')\n",
    "        empty = nf - nf_prev\n",
    "        \n",
    "        rand = torch.randint(nf_prev, (empty,))\n",
    "        expanded = [(i.item(), nf_prev + n) for n, i in enumerate(rand)]\n",
    "        \n",
    "        self.nodes[node].shape = (channel, ) + shape[1:]\n",
    "        self._reconstruct(node, expanded)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = AnchorGraph((3, 32, 32), (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('basic', './transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.deeper_net(Add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('deeper', './transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.add_connection(gr.anchor[0], gr.anchor[1], layer_features=[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7845,  0.6499,  0.4445,  1.1837, -0.2896, -0.9197, -0.7860,  0.7082,\n",
       "         -0.3143,  0.4005]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.generate_model()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('add_connection', './transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_cifar = untar_data(URLs.CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cifar = ImageList.from_folder(path_cifar).split_by_folder(train=\"train\", valid=\"test\").label_from_folder().databunch(bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Conv2d               [64, 16, 16]         1,728      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Add                  [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [64, 16, 16]         36,864     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [64, 16, 16]         128        True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Add                  [64, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Flatten              [16384]              0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [10]                 163,850    True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 424,650\n",
       "Total trainable params: 424,650\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : CrossEntropyLoss\n",
       "======================================================================\n",
       "Callbacks functions applied "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(data_cifar, gr.generate_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.519591</td>\n",
       "      <td>1.863322</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.423097</td>\n",
       "      <td>1.272150</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360294</td>\n",
       "      <td>1.472648</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.297328</td>\n",
       "      <td>1.387930</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.250599</td>\n",
       "      <td>1.424109</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
