{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.graph'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-6f4588680705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mceil_power_of_two\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.graph'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import torch\n",
    "from .graph.graph import Graph\n",
    "from .graph_transformer import *\n",
    "from .function import ceil_power_of_two, next_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from include.graph.graph import Graph\n",
    "from include.graph_transformer import *\n",
    "from include.function import ceil_power_of_two, next_channel\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AnchorGraph(Graph):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(AnchorGraph, self).__init__(input_shape, output_shape)\n",
    "        category = output_shape[0]\n",
    "        \n",
    "        self.anchor = {}\n",
    "        self.convs = []\n",
    "\n",
    "        next = add_conv_block(self, self.input, nf=64, ks=3, st=2)\n",
    "        self.convs.append(next)\n",
    "\n",
    "        next = self.add_anchor(next, layer=Add)\n",
    "        next = add_adaptive_pooling_layer(self, next, target=(1, 1), method='avg')\n",
    "        next = add_flatten_layer(self, next)\n",
    "        next = add_linear_layer(self, next, no=category * 64)\n",
    "        next = add_dropout_layer(self, next, p=0.5)\n",
    "        next = add_linear_layer(self, next, no=category)\n",
    "        self.output = add_dropout_layer(self, next, self.output, p=0.5)\n",
    "        \n",
    "\n",
    "    def _reconstruct(self, target, expanded=[], caller=None):\n",
    "        is_concat = self.nodes[target].layer is Concat\n",
    "        nf = -1 if is_concat else self.nodes[target].shape[0]\n",
    "        out_shape = (nf,) + self.nodes[target].shape[1:]\n",
    "\n",
    "        # Update backwards.\n",
    "        for edge_id in self.nodes[target].in_edge:\n",
    "            edge = self.edges[edge_id]\n",
    "\n",
    "            in_shape = self.nodes[edge.src].shape\n",
    "\n",
    "            if edge.verify_output(in_shape, out_shape):\n",
    "                continue\n",
    "            else:\n",
    "                next_shape, next_expanded = edge.updated_dest(in_shape, out_shape, expanded)\n",
    "                self.nodes[edge.src].shape = next_shape\n",
    "                self._reconstruct(edge.src, next_expanded, target)\n",
    "\n",
    "        # Update self\n",
    "        if is_concat:\n",
    "            cat_channels = 0\n",
    "            offset = 0\n",
    "            new_expanded = []\n",
    "            for edge_id in self.nodes[target].in_edge:\n",
    "                edge = self.edges[edge_id]\n",
    "                num_channels = self.nodes[edge.src].shape[0]\n",
    "                if offset != 0:\n",
    "                    start = cat_channels\n",
    "                    end = cat_channels + num_channels\n",
    "                    new_expanded.extend(\n",
    "                        (i-offset, i) for i in range(start, end)\n",
    "                    )\n",
    "                if caller == edge.src:\n",
    "                    offset = len(expanded)\n",
    "                    new_expanded.extend(\n",
    "                        (-1 if o<0 else o+cat_channels, c+cat_channels) for o, c in expanded\n",
    "                    )\n",
    "                cat_channels += num_channels\n",
    "\n",
    "            self.nodes[target].shape = (cat_channels,) + self.nodes[target].shape[1:]\n",
    "            expanded = new_expanded\n",
    "\n",
    "        # Update forwards.\n",
    "        in_shape = self.nodes[target].shape\n",
    "        for edge_id in self.nodes[target].out_edge:\n",
    "            edge = self.edges[edge_id]\n",
    "\n",
    "            is_concat = self.nodes[edge.dest].layer is Concat\n",
    "            nf = -1 if is_concat else self.nodes[edge.dest].shape[0]\n",
    "            out_shape = (nf,) + self.nodes[edge.dest].shape[1:]\n",
    "            if edge.verify_output(in_shape, out_shape) and not is_concat:\n",
    "                continue\n",
    "            else:\n",
    "                next_shape, next_expanded = edge.updated_src(in_shape, out_shape, expanded)\n",
    "                self.nodes[edge.dest].shape = next_shape\n",
    "                self._reconstruct(edge.dest, next_expanded, target)\n",
    "\n",
    "    def add_anchor(self, src, layer):\n",
    "        anchor = self.insert_node(src, multi_input=True, layer=layer)\n",
    "        anchor_node = self.nodes[anchor]\n",
    "        self.anchor[anchor_node.rank] = anchor\n",
    "        return anchor\n",
    "\n",
    "    def add_connection(self, src, dest, layer_features=[]):\n",
    "        in_shape = self.nodes[src].shape\n",
    "        out_shape = self.nodes[dest].shape\n",
    "        ni, nf = in_shape[0], out_shape[0]\n",
    "\n",
    "        nh = ni\n",
    "        next = src\n",
    "\n",
    "        nl = len(layer_features)\n",
    "        # Generate convolutional layers\n",
    "        for id, layer in enumerate(layer_features):\n",
    "            nh = layer if layer > 0 else nh\n",
    "            next = add_conv_block(self, next, nf=nh, ks=3, st=1,\n",
    "                                  zero_bn = True if id==nl-1 else False)\n",
    "            self.convs.append(next)\n",
    "\n",
    "        # Match the output size\n",
    "        if in_shape[1:] != out_shape[1:]:\n",
    "            shapes = zip(in_shape[1:], out_shape[1:])\n",
    "            kernel = tuple(ceil_power_of_two(i/o) for i, o in shapes)\n",
    "\n",
    "            next = add_pooling_layer(self, next, ks=kernel, method='avg')\n",
    "\n",
    "        # Match the output channel\n",
    "        if self.nodes[dest].layer is Concat:\n",
    "            nf = self.nodes[next].shape[0]\n",
    "\n",
    "            id_edge = IdenticalEdge(next, dest)\n",
    "            self.add_edge(id_edge)\n",
    "            expanded = tuple((-1, x) for x in range(nf))\n",
    "            self._reconstruct(dest, expanded=expanded, caller=next)\n",
    "        elif self.nodes[dest].layer is Add:\n",
    "            if nh != nf:\n",
    "                next = add_conv_layer(self, next,\n",
    "                                      nf=nf, ks=1, bias=True)\n",
    "            id_edge = IdenticalEdge(next, dest)\n",
    "            self.add_edge(id_edge)\n",
    "        return dest\n",
    "\n",
    "    def deeper_net(self, layer):\n",
    "        last_anchor = sorted(self.anchor.items())[-1][1]\n",
    "        in_shape = self.nodes[last_anchor].shape\n",
    "        ni = in_shape[0]\n",
    "        \n",
    "        next = last_anchor\n",
    "        if ni > 512: \n",
    "            edge = AvgPoolingEdge(0, 0, kernel_size=2)\n",
    "            next_shape = edge.calculate_output(in_shape)\n",
    "            next = self.insert_node(next, shape=next_shape, edge=edge)\n",
    "        \n",
    "        self.add_anchor(next, layer=layer)\n",
    "        edge = ConvEdge(0, 0, in_channels=ni, out_channels=ni, kernel_size=3, bias=False)\n",
    "        edge.set_identical()\n",
    "        next = self.insert_node(next, edge=edge)\n",
    "        self.convs.append(next)\n",
    "        edge = BatchNormEdge(0, 0, ni)\n",
    "        edge.set_identical()\n",
    "        next = self.insert_node(next, edge=edge)\n",
    "        next = self.insert_node(next, edge=ReluEdge(0, 0))\n",
    "        \n",
    "        return next\n",
    "\n",
    "    def wider_net(self, node):\n",
    "        ni = self.nodes[node].shape[0]\n",
    "        next_ni = next_channel(ni)\n",
    "        self.expand_channel(node, next_ni)\n",
    "\n",
    "    def expand_channel(self, node, channel):\n",
    "        shape = self.nodes[node].shape\n",
    "        nf_prev = shape[0]\n",
    "        nf = channel\n",
    "        if nf_prev > nf: raise Exception('Trying expand to lower channels')\n",
    "        if nf_prev == nf: return\n",
    "        empty = nf - nf_prev\n",
    "\n",
    "        rand = torch.randint(nf_prev, (empty,))\n",
    "        expanded = [(i.item(), nf_prev + n) for n, i in enumerate(rand)]\n",
    "\n",
    "        self.nodes[node].shape = (channel, ) + shape[1:]\n",
    "        self._reconstruct(node, expanded)\n",
    "        return\n",
    "    \n",
    "    def save(self, dir_name):\n",
    "        graph_data={\n",
    "            'anchor': self.anchor,\n",
    "            'convs': self.convs\n",
    "        }\n",
    "        super().save(dir_name, graph_data=graph_data)\n",
    "    \n",
    "    def load(self, dir_name):\n",
    "        graph_data, weight_data, key_to_id = super().load(dir_name)\n",
    "        anchors = graph_data['anchor']\n",
    "        convs = graph_data['convs']\n",
    "            \n",
    "        self.anchor = dict([(k, key_to_id[i]) for k, i in anchors.items()])\n",
    "        self.convs = [key_to_id[k] for k in convs]\n",
    "        return graph_data, weight_data, key_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted anchored_graph.ipynb to exp/nb_anchored_graph.py\r\n"
     ]
    }
   ],
   "source": [
    "!python nb2py.py anchored_graph.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = AnchorGraph((3, 32, 32), (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.deeper_net(Concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('deeper', 'transform/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target13\n",
      "target8\n",
      "target9\n",
      "target10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.add_connection(gr.anchor[1], gr.anchor[2], layer_features=[64, 128, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('skip', 'transform/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 14, 20, 24, 28]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target28\n",
      "target27\n",
      "target26\n",
      "target25\n",
      "target24\n",
      "target13\n",
      "target8\n",
      "target9\n",
      "target10\n"
     ]
    }
   ],
   "source": [
    "gr.wider_net(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target28\n",
      "target27\n",
      "target26\n",
      "target25\n",
      "target24\n",
      "target13\n",
      "target8\n",
      "target9\n",
      "target10\n"
     ]
    }
   ],
   "source": [
    "gr.wider_net(28)\n",
    "gr.wider_net(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('wider', 'transform/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.deeper_net(Concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.visualize('deeper_again', './transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gr.save('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr2 = AnchorGraph((3, 32, 32), (10,))\n",
    "_ = gr2.load('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 14, 20, 24, 28, 31]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr2.convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cifar = untar_data(URLs.CIFAR)\n",
    "data_cifar = ImageList.from_folder(path_cifar).split_by_folder(train=\"train\", valid=\"test\").label_from_folder().databunch(bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data_cifar, gr2.generate_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.582388</td>\n",
       "      <td>1.486299</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.340784</td>\n",
       "      <td>1.526956</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.249703</td>\n",
       "      <td>1.439815</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.150209</td>\n",
       "      <td>1.164656</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.086445</td>\n",
       "      <td>1.029783</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict([(1, 'a'), (3, 'b')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
